{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "coursera": {
      "course_slug": "nlp-sequence-models",
      "graded_item_id": "RNnEs",
      "launcher_item_id": "acNYU"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Emojify.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hvauchar/Natural-Language-Processing-Projects/blob/master/Emojify.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ztMbTm9pJwD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from emo_utils import *\n",
        "import emoji\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZCQhFFupJwZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, Y_train = read_csv('data/train_emoji.csv')\n",
        "X_test, Y_test = read_csv('data/tesss.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0k3Cu6-pJwk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "maxLen = len(max(X_train, key=len).split())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZyL4O9K0pJwy",
        "colab_type": "code",
        "colab": {},
        "outputId": "544249e4-930f-465c-c2d7-cc999283e25c"
      },
      "source": [
        "index = 1\n",
        "print(X_train[index], label_to_emoji(Y_train[index]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I am proud of your achievements üòÑ\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImIkwZMbpJxF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y_oh_train = convert_to_one_hot(Y_train, C = 5)\n",
        "Y_oh_test = convert_to_one_hot(Y_test, C = 5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwLR-zNCpJxT",
        "colab_type": "code",
        "colab": {},
        "outputId": "c6383bb6-1128-4e9f-bf68-d5e1872e2594"
      },
      "source": [
        "index = 50\n",
        "print(Y_train[index], \"is converted into one hot\", Y_oh_train[index])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 is converted into one hot [ 1.  0.  0.  0.  0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ruvwnVGNpJxk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_to_index, index_to_word, word_to_vec_map = read_glove_vecs('data/glove.6B.50d.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zn6kHtPbpJxw",
        "colab_type": "code",
        "colab": {},
        "outputId": "1e70d4bd-eb87-49df-93ab-1bde148bcb96"
      },
      "source": [
        "word = \"cucumber\"\n",
        "index = 289846\n",
        "print(\"the index of\", word, \"in the vocabulary is\", word_to_index[word])\n",
        "print(\"the\", str(index) + \"th word in the vocabulary is\", index_to_word[index])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the index of cucumber in the vocabulary is 113317\n",
            "the 289846th word in the vocabulary is potatos\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fx2ro9lFpJx8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sentence_to_avg(sentence, word_to_vec_map):\n",
        "    # Step 1: Split sentence into list of lower case words (‚âà 1 line)\n",
        "    words = sentence.lower().split()\n",
        "\n",
        "    # Initialize the average word vector, should have the same shape as your word vectors.\n",
        "    avg = np.zeros(word_to_vec_map[words[0]].shape)\n",
        "    \n",
        "    # Step 2: average the word vectors. You can loop over the words in the list \"words\".\n",
        "    for w in words:\n",
        "        avg += word_to_vec_map[w]\n",
        "    avg = avg/len(words)\n",
        "    \n",
        "    return avg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "eZRyDyeypJyN",
        "colab_type": "code",
        "colab": {},
        "outputId": "44571a94-dbdc-4d07-a088-d76bd6bfd258"
      },
      "source": [
        "avg = sentence_to_avg(\"Morrocan couscous is my favorite dish\", word_to_vec_map)\n",
        "print(\"avg = \", avg)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "avg =  [-0.008005    0.56370833 -0.50427333  0.258865    0.55131103  0.03104983\n",
            " -0.21013718  0.16893933 -0.09590267  0.141784   -0.15708967  0.18525867\n",
            "  0.6495785   0.38371117  0.21102167  0.11301667  0.02613967  0.26037767\n",
            "  0.05820667 -0.01578167 -0.12078833 -0.02471267  0.4128455   0.5152061\n",
            "  0.38756167 -0.898661   -0.535145    0.33501167  0.68806933 -0.2156265\n",
            "  1.797155    0.10476933 -0.36775333  0.750785    0.10282583  0.348925\n",
            " -0.27262833  0.66768    -0.10706167 -0.283635    0.59580117  0.28747333\n",
            " -0.3366635   0.23393817  0.34349183  0.178405    0.1166155  -0.076433\n",
            "  0.1445417   0.09808667]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYXDhxOQpJyY",
        "colab_type": "text"
      },
      "source": [
        "**Expected Output**:\n",
        "\n",
        "<table>\n",
        "    <tr>\n",
        "        <td>\n",
        "            **avg= **\n",
        "        </td>\n",
        "        <td>\n",
        "           [-0.008005    0.56370833 -0.50427333  0.258865    0.55131103  0.03104983\n",
        " -0.21013718  0.16893933 -0.09590267  0.141784   -0.15708967  0.18525867\n",
        "  0.6495785   0.38371117  0.21102167  0.11301667  0.02613967  0.26037767\n",
        "  0.05820667 -0.01578167 -0.12078833 -0.02471267  0.4128455   0.5152061\n",
        "  0.38756167 -0.898661   -0.535145    0.33501167  0.68806933 -0.2156265\n",
        "  1.797155    0.10476933 -0.36775333  0.750785    0.10282583  0.348925\n",
        " -0.27262833  0.66768    -0.10706167 -0.283635    0.59580117  0.28747333\n",
        " -0.3366635   0.23393817  0.34349183  0.178405    0.1166155  -0.076433\n",
        "  0.1445417   0.09808667]\n",
        "        </td>\n",
        "    </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "O3S2t46IpJzC",
        "colab_type": "code",
        "colab": {},
        "outputId": "da9f97d5-c472-491b-f72f-d6f8c6f95381"
      },
      "source": [
        "print(\"Training set:\")\n",
        "pred_train = predict(X_train, Y_train, W, b, word_to_vec_map)\n",
        "print('Test set:')\n",
        "pred_test = predict(X_test, Y_test, W, b, word_to_vec_map)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training set:\n",
            "Accuracy: 0.977272727273\n",
            "Test set:\n",
            "Accuracy: 0.857142857143\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0eIJSfa9pJzS",
        "colab_type": "code",
        "colab": {},
        "outputId": "5f605e20-3185-496a-9647-d6b3148fa675"
      },
      "source": [
        "X_my_sentences = np.array([\"i adore you\", \"i love you\", \"funny lol\", \"lets play with a ball\", \"food is ready\", \"not feeling happy\"])\n",
        "Y_my_labels = np.array([[0], [0], [2], [1], [4],[3]])\n",
        "\n",
        "pred = predict(X_my_sentences, Y_my_labels , W, b, word_to_vec_map)\n",
        "print_predictions(X_my_sentences, pred)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.833333333333\n",
            "\n",
            "i adore you ‚ù§Ô∏è\n",
            "i love you ‚ù§Ô∏è\n",
            "funny lol üòÑ\n",
            "lets play with a ball ‚öæ\n",
            "food is ready üç¥\n",
            "not feeling happy üòÑ\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hAd-_l9pJzh",
        "colab_type": "code",
        "colab": {},
        "outputId": "2fe3b99a-d1c8-4164-c836-63114eedbdad"
      },
      "source": [
        "print(Y_test.shape)\n",
        "print('           '+ label_to_emoji(0)+ '    ' + label_to_emoji(1) + '    ' +  label_to_emoji(2)+ '    ' + label_to_emoji(3)+'   ' + label_to_emoji(4))\n",
        "print(pd.crosstab(Y_test, pred_test.reshape(56,), rownames=['Actual'], colnames=['Predicted'], margins=True))\n",
        "plot_confusion_matrix(Y_test, pred_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(56,)\n",
            "           ‚ù§Ô∏è    ‚öæ    üòÑ    üòû   üç¥\n",
            "Predicted  0.0  1.0  2.0  3.0  4.0  All\n",
            "Actual                                 \n",
            "0            6    0    0    1    0    7\n",
            "1            0    8    0    0    0    8\n",
            "2            2    0   16    0    0   18\n",
            "3            1    1    2   12    0   16\n",
            "4            0    0    1    0    6    7\n",
            "All          9    9   19   13    6   56\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQwAAAD3CAYAAADormr9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f159d6084a8>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__gQHCAXpJzx",
        "colab_type": "code",
        "colab": {},
        "outputId": "8170e980-424b-468d-d75c-219539b21c02"
      },
      "source": [
        "import numpy as np\n",
        "np.random.seed(0)\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, Input, Dropout, LSTM, Activation\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.preprocessing import sequence\n",
        "from keras.initializers import glorot_uniform\n",
        "np.random.seed(1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4NuPKljfpJ0y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Emojify_V2(input_shape, word_to_vec_map, word_to_index):\n",
        "    # Define sentence_indices as the input of the graph, it should be of shape input_shape and dtype 'int32' (as it contains indices).\n",
        "    sentence_indices = Input(shape=input_shape)\n",
        "    \n",
        "    # Create the embedding layer pretrained with GloVe Vectors (‚âà1 line)\n",
        "    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
        "    \n",
        "    # Propagate sentence_indices through your embedding layer, you get back the embeddings\n",
        "    embeddings =  embedding_layer(sentence_indices)  \n",
        "    \n",
        "    # Propagate the embeddings through an LSTM layer with 128-dimensional hidden state\n",
        "    # Be careful, the returned output should be a batch of sequences.\n",
        "    X = LSTM(128 ,return_sequences=True)(embeddings)\n",
        "    # Add dropout with a probability of 0.5\n",
        "    X = Dropout(0.5)(X)\n",
        "    # Propagate X trough another LSTM layer with 128-dimensional hidden state\n",
        "    # Be careful, the returned output should be a single hidden state, not a batch of sequences.\n",
        "    X = LSTM(128,return_sequences=False)(X)\n",
        "    # Add dropout with a probability of 0.5\n",
        "    X = Dropout(0.5)(X)\n",
        "    # Propagate X through a Dense layer with softmax activation to get back a batch of 5-dimensional vectors.\n",
        "    X = Dense(5)(X)\n",
        "    # Add a softmax activation\n",
        "    X = Activation('softmax')(X)\n",
        "    \n",
        "    # Create Model instance which converts sentence_indices into X.\n",
        "    model = Model(inputs=sentence_indices, outputs=X)\n",
        "   return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "J68FE052pJ0-",
        "colab_type": "code",
        "colab": {},
        "outputId": "ee990532-5419-4708-ba51-17f54801fe24"
      },
      "source": [
        "model = Emojify_V2((maxLen,), word_to_vec_map, word_to_index)\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "embedding_3 (Embedding)      (None, 10, 50)            20000050  \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 10, 128)           91648     \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 10, 128)           0         \n",
            "_________________________________________________________________\n",
            "lstm_4 (LSTM)                (None, 128)               131584    \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 5)                 645       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 5)                 0         \n",
            "=================================================================\n",
            "Total params: 20,223,927\n",
            "Trainable params: 223,877\n",
            "Non-trainable params: 20,000,050\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-yF26tVpJ1M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DsR4pEXpJ1W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_indices = sentences_to_indices(X_train, word_to_index, maxLen)\n",
        "Y_train_oh = convert_to_one_hot(Y_train, C = 5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "a9stZDAnpJ1k",
        "colab_type": "code",
        "colab": {},
        "outputId": "b59d900f-6fdf-4796-c2d5-d2366f3adc72"
      },
      "source": [
        "model.fit(X_train_indices, Y_train_oh, epochs = 50, batch_size = 32, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "132/132 [==============================] - 0s - loss: 1.6068 - acc: 0.1818     \n",
            "Epoch 2/50\n",
            "132/132 [==============================] - 0s - loss: 1.5374 - acc: 0.3030     \n",
            "Epoch 3/50\n",
            "132/132 [==============================] - 0s - loss: 1.4748 - acc: 0.3788     \n",
            "Epoch 4/50\n",
            "132/132 [==============================] - 0s - loss: 1.4244 - acc: 0.4470     \n",
            "Epoch 5/50\n",
            "132/132 [==============================] - 0s - loss: 1.3228 - acc: 0.4848     \n",
            "Epoch 6/50\n",
            "132/132 [==============================] - 0s - loss: 1.1742 - acc: 0.5682     \n",
            "Epoch 7/50\n",
            "132/132 [==============================] - 0s - loss: 1.0320 - acc: 0.6136     \n",
            "Epoch 8/50\n",
            "132/132 [==============================] - 0s - loss: 0.9505 - acc: 0.6894     \n",
            "Epoch 9/50\n",
            "132/132 [==============================] - 0s - loss: 0.9397 - acc: 0.6742     \n",
            "Epoch 10/50\n",
            "132/132 [==============================] - 0s - loss: 0.7387 - acc: 0.7121     \n",
            "Epoch 11/50\n",
            "132/132 [==============================] - 0s - loss: 0.6654 - acc: 0.7879     \n",
            "Epoch 12/50\n",
            "132/132 [==============================] - 0s - loss: 0.7054 - acc: 0.7652     \n",
            "Epoch 13/50\n",
            "132/132 [==============================] - 0s - loss: 0.5361 - acc: 0.8106     \n",
            "Epoch 14/50\n",
            "132/132 [==============================] - 0s - loss: 0.4495 - acc: 0.8485     \n",
            "Epoch 15/50\n",
            "132/132 [==============================] - 0s - loss: 0.4880 - acc: 0.8182     \n",
            "Epoch 16/50\n",
            "132/132 [==============================] - 0s - loss: 0.5286 - acc: 0.7955     \n",
            "Epoch 17/50\n",
            "132/132 [==============================] - 0s - loss: 0.3885 - acc: 0.8788     \n",
            "Epoch 18/50\n",
            "132/132 [==============================] - 0s - loss: 0.2983 - acc: 0.9015     \n",
            "Epoch 19/50\n",
            "132/132 [==============================] - 0s - loss: 0.2607 - acc: 0.9091     \n",
            "Epoch 20/50\n",
            "132/132 [==============================] - 0s - loss: 0.2961 - acc: 0.9167     \n",
            "Epoch 21/50\n",
            "132/132 [==============================] - 0s - loss: 0.3153 - acc: 0.9015     \n",
            "Epoch 22/50\n",
            "132/132 [==============================] - 0s - loss: 0.2113 - acc: 0.9167     \n",
            "Epoch 23/50\n",
            "132/132 [==============================] - 0s - loss: 0.1718 - acc: 0.9318     \n",
            "Epoch 24/50\n",
            "132/132 [==============================] - 0s - loss: 0.2098 - acc: 0.9091     \n",
            "Epoch 25/50\n",
            "132/132 [==============================] - 0s - loss: 0.2365 - acc: 0.9015     \n",
            "Epoch 26/50\n",
            "132/132 [==============================] - 0s - loss: 0.1515 - acc: 0.9470     \n",
            "Epoch 27/50\n",
            "132/132 [==============================] - 0s - loss: 0.2408 - acc: 0.9242     \n",
            "Epoch 28/50\n",
            "132/132 [==============================] - 0s - loss: 0.3497 - acc: 0.9015     \n",
            "Epoch 29/50\n",
            "132/132 [==============================] - 0s - loss: 0.2756 - acc: 0.9091     \n",
            "Epoch 30/50\n",
            "132/132 [==============================] - 0s - loss: 0.2702 - acc: 0.9242     \n",
            "Epoch 31/50\n",
            "132/132 [==============================] - 0s - loss: 0.1496 - acc: 0.9621     \n",
            "Epoch 32/50\n",
            "132/132 [==============================] - 0s - loss: 0.1264 - acc: 0.9621     \n",
            "Epoch 33/50\n",
            "132/132 [==============================] - 0s - loss: 0.1108 - acc: 0.9773     \n",
            "Epoch 34/50\n",
            "132/132 [==============================] - 0s - loss: 0.1429 - acc: 0.9621     \n",
            "Epoch 35/50\n",
            "132/132 [==============================] - 0s - loss: 0.0620 - acc: 0.9848     \n",
            "Epoch 36/50\n",
            "132/132 [==============================] - 0s - loss: 0.0533 - acc: 0.9848     \n",
            "Epoch 37/50\n",
            "132/132 [==============================] - 0s - loss: 0.0595 - acc: 0.9848     \n",
            "Epoch 38/50\n",
            "132/132 [==============================] - 0s - loss: 0.0281 - acc: 1.0000     \n",
            "Epoch 39/50\n",
            "132/132 [==============================] - 0s - loss: 0.0816 - acc: 0.9773     \n",
            "Epoch 40/50\n",
            "132/132 [==============================] - 0s - loss: 0.0413 - acc: 0.9924     \n",
            "Epoch 41/50\n",
            "132/132 [==============================] - 0s - loss: 0.0395 - acc: 0.9848     \n",
            "Epoch 42/50\n",
            "132/132 [==============================] - 0s - loss: 0.0299 - acc: 1.0000     \n",
            "Epoch 43/50\n",
            "132/132 [==============================] - 0s - loss: 0.0212 - acc: 1.0000     \n",
            "Epoch 44/50\n",
            "132/132 [==============================] - 0s - loss: 0.0219 - acc: 1.0000     \n",
            "Epoch 45/50\n",
            "132/132 [==============================] - 0s - loss: 0.0071 - acc: 1.0000     \n",
            "Epoch 46/50\n",
            "132/132 [==============================] - 0s - loss: 0.0166 - acc: 0.9924     \n",
            "Epoch 47/50\n",
            "132/132 [==============================] - 0s - loss: 0.0056 - acc: 1.0000     \n",
            "Epoch 48/50\n",
            "132/132 [==============================] - 0s - loss: 0.0139 - acc: 0.9924     \n",
            "Epoch 49/50\n",
            "132/132 [==============================] - 0s - loss: 0.0073 - acc: 1.0000     \n",
            "Epoch 50/50\n",
            "132/132 [==============================] - 0s - loss: 0.0073 - acc: 1.0000     \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f1585585fd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "iFKpm90mpJ12",
        "colab_type": "code",
        "colab": {},
        "outputId": "d231939d-3c2b-4b78-e3bb-e2e6f01ea7f3"
      },
      "source": [
        "X_test_indices = sentences_to_indices(X_test, word_to_index, max_len = maxLen)\n",
        "Y_test_oh = convert_to_one_hot(Y_test, C = 5)\n",
        "loss, acc = model.evaluate(X_test_indices, Y_test_oh)\n",
        "print()\n",
        "print(\"Test accuracy = \", acc)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "32/56 [================>.............] - ETA: 0s\n",
            "Test accuracy =  0.839285705771\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pn_6zH2jpJ1_",
        "colab_type": "code",
        "colab": {},
        "outputId": "54e5f7bb-8a13-4cc5-961b-b6de23f60688"
      },
      "source": [
        "# This code allows you to see the mislabelled examples\n",
        "C = 5\n",
        "y_test_oh = np.eye(C)[Y_test.reshape(-1)]\n",
        "X_test_indices = sentences_to_indices(X_test, word_to_index, maxLen)\n",
        "pred = model.predict(X_test_indices)\n",
        "for i in range(len(X_test)):\n",
        "    x = X_test_indices\n",
        "    num = np.argmax(pred[i])\n",
        "    if(num != Y_test[i]):\n",
        "        print('Expected emoji:'+ label_to_emoji(Y_test[i]) + ' prediction: '+ X_test[i] + label_to_emoji(num).strip())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Expected emoji:üòÑ prediction: she got me a nice present\t‚ù§Ô∏è\n",
            "Expected emoji:üòû prediction: work is horrible\tüòÑ\n",
            "Expected emoji:üç¥ prediction: any suggestions for dinner\tüòÑ\n",
            "Expected emoji:‚ù§Ô∏è prediction: I love taking breaks\tüòû\n",
            "Expected emoji:üòÑ prediction: you brighten my day\t‚ù§Ô∏è\n",
            "Expected emoji:üòÑ prediction: will you be my valentine\t‚ù§Ô∏è\n",
            "Expected emoji:‚ù§Ô∏è prediction: I love you to the stars and back\tüòÑ\n",
            "Expected emoji:üòû prediction: go away\t‚öæ\n",
            "Expected emoji:üç¥ prediction: I did not have breakfast ‚ù§Ô∏è\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WuRODKtBpJ2H",
        "colab_type": "text"
      },
      "source": [
        "Now you can try it on your own example. Write your own sentence below. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfsDsrYCpJ2I",
        "colab_type": "code",
        "colab": {},
        "outputId": "f6214bc1-7e7c-4ba2-f7d4-c32dcc7103d1"
      },
      "source": [
        "# Change the sentence below to see your prediction. Make sure all the words are in the Glove embeddings.  \n",
        "x_test = np.array(['I won the prize'])\n",
        "X_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\n",
        "print(x_test[0] +' '+  label_to_emoji(np.argmax(model.predict(X_test_indices))))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I won the prize üòÑ\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}